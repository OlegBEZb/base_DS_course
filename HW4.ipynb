{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HhA6jSmagH5e"
   },
   "source": [
    "# Loading the needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qMuQFLjZgH5g"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A-ICvRStgH5i"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA as iPCA, TruncatedSVD\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss, make_scorer\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ZSTeq0NgH5l"
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "import nltk\n",
    "\n",
    "from collections import Counter\n",
    "import string\n",
    "from textblob import TextBlob, Word\n",
    "from random import shuffle\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from eli5.lime import TextExplainer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "from sklearn.externals.joblib import Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eeiQyXv_gH5n"
   },
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uXt8oKltgH5o"
   },
   "source": [
    "Checking the path we are working in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XP9UeNScgH5p",
    "outputId": "bbe47e8d-ae32-4ea5-99ca-bea01ffbc26b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Oleg\\\\Desktop\\\\Универ\\\\EPAM\\\\DS\\\\Homework\\\\HW4'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oU28A6xqgH5t"
   },
   "outputs": [],
   "source": [
    "HW4_path=os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODcVW8mkgH5v"
   },
   "source": [
    "Finding dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3DOgk6ZbgH5w",
    "outputId": "4a8aecfd-6e9a-4a5d-c859-9e128b3cc15a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', '04 Classification – Homework.docx', 'Dataset', 'HW4.ipynb', '~$ Classification – Homework.docx']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2qNgxpQ7gH51"
   },
   "source": [
    "Creating paths to simplify movements between folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dJzNm_39gH51"
   },
   "outputs": [],
   "source": [
    "data_path=HW4_path+\"\\\\Dataset\"\n",
    "\n",
    "train_data_path=data_path+\"\\\\train\"\n",
    "test_data_path=data_path+\"\\\\test\"\n",
    "\n",
    "pos_train_data_path=train_data_path+\"\\\\pos\"\n",
    "neg_train_data_path=train_data_path+\"\\\\neg\"\n",
    "\n",
    "pos_test_data_path=test_data_path+\"\\\\pos\"\n",
    "neg_test_data_path=test_data_path+\"\\\\neg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lmytqJsRgH54"
   },
   "source": [
    "Load user reviews and their grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aMEBimgPgH54"
   },
   "outputs": [],
   "source": [
    "os.chdir(pos_train_data_path)\n",
    "pos_train_data=[]\n",
    "pos_train_labels=[]\n",
    "\n",
    "for textfile in os.listdir(pos_train_data_path):\n",
    "    file = open( textfile, 'r', encoding=\"utf8\")\n",
    "    pos_train_data.append( file.read())\n",
    "    \n",
    "    _,label = textfile.split('_')\n",
    "    label=int(label[0])\n",
    "    pos_train_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCwWe_oogH57"
   },
   "outputs": [],
   "source": [
    "os.chdir(neg_train_data_path)\n",
    "neg_train_data=[]\n",
    "neg_train_labels=[]\n",
    "\n",
    "for textfile in os.listdir(neg_train_data_path):\n",
    "    file = open( textfile, 'r', encoding=\"utf8\")\n",
    "    neg_train_data.append( file.read())\n",
    "    \n",
    "    _,label = textfile.split('_')\n",
    "    label=int(label[0])\n",
    "    neg_train_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rkv5LgVUgH5-"
   },
   "outputs": [],
   "source": [
    "os.chdir(pos_test_data_path)\n",
    "pos_test_data=[]\n",
    "pos_test_labels=[]\n",
    "\n",
    "for textfile in os.listdir(pos_test_data_path):\n",
    "    file = open( textfile, 'r', encoding=\"utf8\")\n",
    "    pos_test_data.append( file.read())\n",
    "    \n",
    "    _,label = textfile.split('_')\n",
    "    label=int(label[0])\n",
    "    pos_test_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkT31YU-gH6C"
   },
   "outputs": [],
   "source": [
    "os.chdir(neg_test_data_path)\n",
    "neg_test_data=[]\n",
    "neg_test_labels=[]\n",
    "\n",
    "for textfile in os.listdir(neg_test_data_path):\n",
    "    file = open( textfile, 'r', encoding=\"utf8\")\n",
    "    neg_test_data.append( file.read())\n",
    "    \n",
    "    _,label = textfile.split('_')\n",
    "    label=int(label[0])\n",
    "    neg_test_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YrM-e0UJgH6F"
   },
   "outputs": [],
   "source": [
    "pos_train_data_pd = pd.DataFrame(data=pos_train_data, columns=['review'])\n",
    "neg_train_data_pd = pd.DataFrame(data=neg_train_data, columns=['review'])\n",
    "\n",
    "pos_test_data_pd = pd.DataFrame(data=pos_test_data, columns=['review'])\n",
    "neg_test_data_pd = pd.DataFrame(data=neg_test_data, columns=['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wgktDBHOgH6H"
   },
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-7z-lLHgH6I"
   },
   "source": [
    "#### Number of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kDlUcLYygH6K"
   },
   "source": [
    "Check if number of words in review can predict the grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PPj1XdwmgH6K",
    "outputId": "3f334c5d-5d2b-4da9-e5ad-26f8d22e8394"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words in review :  236.69568\n"
     ]
    }
   ],
   "source": [
    "pos_train_data_pd['word_count'] = pos_train_data_pd['review'].apply(lambda x: len(str(x).split(\" \")))\n",
    "print('Average number of words in review : ', pos_train_data_pd['word_count'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1i4YSjFPgH6N",
    "outputId": "b14a4da8-7cc2-4f07-b2df-978a19a51fed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words in review :  230.85776\n"
     ]
    }
   ],
   "source": [
    "neg_train_data_pd['word_count'] = neg_train_data_pd['review'].apply(lambda x: len(str(x).split(\" \")))\n",
    "print('Average number of words in review : ', neg_train_data_pd['word_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgRsT7hlgH6P"
   },
   "source": [
    "For negative and positive comments number of words is almost identical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WgmVUTwxgH6R"
   },
   "source": [
    "#### Average word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "flTc1U1UgH6R",
    "outputId": "e4dd8b67-8006-4f93-e685-f499418459f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length in review :  4.660274983731835\n"
     ]
    }
   ],
   "source": [
    "def avg_word(review):\n",
    "    words = review.split()\n",
    "    return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "pos_train_data_pd['avg_word'] = pos_train_data_pd['review'].apply(lambda x: avg_word(x))\n",
    "print('Average word length in review : ', pos_train_data_pd['avg_word'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F4R7NkYmgH6T",
    "outputId": "8b9e43b2-ed74-4a16-e00d-ac46a7c03744"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length in review :  4.627199126537517\n"
     ]
    }
   ],
   "source": [
    "neg_train_data_pd['avg_word'] = neg_train_data_pd['review'].apply(lambda x: avg_word(x))\n",
    "print('Average word length in review : ', neg_train_data_pd['avg_word'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yBrCvcQWgH6W"
   },
   "source": [
    "There is no difference here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6zkxIvwgH6X"
   },
   "source": [
    "#### Number of stopwords\n",
    "\n",
    "Before changing and removing the stopwords let's try to find some patterns with default list of stopwords from NLTK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UObaMaVogH6X",
    "outputId": "ac31e965-5d8a-4956-c345-f13cca37a39d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of stopwords in review :  97.12576\n",
      "Average part of stopwords in review :  0.410340230966615\n"
     ]
    }
   ],
   "source": [
    "pos_train_data_pd['stopwords'] = pos_train_data_pd['review'].apply(lambda x: len([x for x in x.split() if x in cachedStopWords]))\n",
    "print('Average number of stopwords in review : ', pos_train_data_pd['stopwords'].mean())\n",
    "print('Average part of stopwords in review : ', pos_train_data_pd['stopwords'].mean() / pos_train_data_pd['word_count'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ECKTBc5ngH6a",
    "outputId": "f8358010-d247-4132-9d2d-1c05ce260b18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of stopwords in review :  94.63528\n",
      "Average part of stopwords in review :  0.4099289536552724\n"
     ]
    }
   ],
   "source": [
    "neg_train_data_pd['stopwords'] = neg_train_data_pd['review'].apply(lambda x: len([x for x in x.split() if x in cachedStopWords]))\n",
    "print('Average number of stopwords in review : ', neg_train_data_pd['stopwords'].mean())\n",
    "print('Average part of stopwords in review : ', neg_train_data_pd['stopwords'].mean() / neg_train_data_pd['word_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0uUVVZZsgH6d"
   },
   "source": [
    "And now there is nothing notable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TJ7Hun2lgH6e"
   },
   "source": [
    "#### Number of swear words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l8ULLloogH6f",
    "outputId": "cb194816-6014-46a6-e119-6887ad99436d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of swear_words in review :  0.01936\n",
      "Average part of swear_words in review :  8.179278979658606e-05\n"
     ]
    }
   ],
   "source": [
    "#collecting swear words (shifted to the right) and present them in two cases: low and with capital letter\n",
    "swear_words=[                                                                                                                                        'Bastard', 'Beaver', 'Bellend', 'Bloodclaat', 'Clunge', 'Cock', 'Dick', 'Dickhead', 'Fanny', 'Flaps', 'Gash', 'Knob', 'Minge', 'Prick', 'Punani', 'Pussy', 'Snatch', 'Twat', 'Cunt', 'Fuck', 'Motherfucker', 'Arsehole', 'Balls', 'Bint', 'Bitch', 'Bollocks', 'Bullshit', 'Feck', 'Munter', 'pissed off', 'Shit', 'Son of a bitch', 'Tits']\n",
    "swear_words += [word.lower() for word in swear_words]\n",
    "\n",
    "pos_train_data_pd['swear_words'] = pos_train_data_pd['review'].apply(lambda x: len([x for x in x.split() if x in swear_words]))\n",
    "print('Average number of swear_words in review : ', pos_train_data_pd['swear_words'].mean())\n",
    "print('Average part of swear_words in review : ', pos_train_data_pd['swear_words'].mean() / pos_train_data_pd['word_count'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lNZOzdzHgH6h",
    "outputId": "9d7440a6-c000-4120-f80a-151846793a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of swear_words in review :  0.0168\n",
      "Average part of swear_words in review :  7.277208268849181e-05\n"
     ]
    }
   ],
   "source": [
    "neg_train_data_pd['swear_words'] = neg_train_data_pd['review'].apply(lambda x: len([x for x in x.split() if x in swear_words]))\n",
    "print('Average number of swear_words in review : ', neg_train_data_pd['swear_words'].mean())\n",
    "print('Average part of swear_words in review : ', neg_train_data_pd['swear_words'].mean() / neg_train_data_pd['word_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MSTcxXUWgH6j"
   },
   "source": [
    "Both categories have similar frequencies (very small) of swear words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rxVs0fnagH6k"
   },
   "source": [
    "#### Number of numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fJbzGrmtgH6l",
    "outputId": "8464378d-c81f-4601-8c3f-4a48c822f12a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of numerics in review :  0.5392\n",
      "Average part of numerics in review :  0.0022780305918553307\n"
     ]
    }
   ],
   "source": [
    "pos_train_data_pd['numerics'] = pos_train_data_pd['review'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "print('Average number of numerics in review : ', pos_train_data_pd['numerics'].mean())\n",
    "print('Average part of numerics in review : ', pos_train_data_pd['numerics'].mean() / pos_train_data_pd['word_count'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KPqTHLaBgH6n",
    "outputId": "7bf2922c-021b-4d6a-a3c3-531bed8485f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of numerics in review :  0.61936\n",
      "Average part of numerics in review :  0.0026828641151157317\n"
     ]
    }
   ],
   "source": [
    "neg_train_data_pd['numerics'] = neg_train_data_pd['review'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "print('Average number of numerics in review : ', neg_train_data_pd['numerics'].mean())\n",
    "print('Average part of numerics in review : ', neg_train_data_pd['numerics'].mean() / neg_train_data_pd['word_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lS0cLH2HgH6p"
   },
   "source": [
    "Parts of numerics in reviews are small and that there is no visible differense betweed categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3aWkJtYPgH6r"
   },
   "source": [
    "#### Number of Uppercase words (CAPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nXLAUUNggH6r",
    "outputId": "548447cc-aded-4278-8518-ca725f96e850"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of upper words in review :  4.53352\n",
      "Average part of upper words in review :  0.019153370268523703\n"
     ]
    }
   ],
   "source": [
    "pos_train_data_pd['upper'] = pos_train_data_pd['review'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "print('Average number of upper words in review : ', pos_train_data_pd['upper'].mean())\n",
    "print('Average part of upper words in review : ', pos_train_data_pd['upper'].mean() / pos_train_data_pd['word_count'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c8QvgakKgH6u",
    "outputId": "af2becab-e9f5-44e6-f2cb-036143d59fa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of upper words in review :  5.16712\n",
      "Average part of upper words in review :  0.022382266898890467\n"
     ]
    }
   ],
   "source": [
    "neg_train_data_pd['upper'] = neg_train_data_pd['review'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "print('Average number of upper words in review : ', neg_train_data_pd['upper'].mean())\n",
    "print('Average part of upper words in review : ', neg_train_data_pd['upper'].mean() / neg_train_data_pd['word_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8EY2C8xOgH6y"
   },
   "source": [
    "And now there is nothing suspicious"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1A_wXTG2gH6z"
   },
   "source": [
    "#### Number of punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lDgm0QR0gH6z",
    "outputId": "27d27a7f-d368-4833-e57c-21263ef5a100"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of punctuation marks in review :  36.55952\n"
     ]
    }
   ],
   "source": [
    "punctuation_marks = ['...', ',', '?', '!', ':', ';', '\"', '\\'', '-', '.', '–', '—']\n",
    "\n",
    "pos_train_data_pd['punctuation_marks'] = pos_train_data_pd['review'].apply(lambda x: sum([1 for x in x if x in punctuation_marks]))\n",
    "print('Average number of punctuation marks in review : ', pos_train_data_pd['punctuation_marks'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J654zYTbgH62",
    "outputId": "1ae2f059-9f96-4b4b-fb5f-53b3a4867989"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of punctuation marks in review :  37.66912\n"
     ]
    }
   ],
   "source": [
    "neg_train_data_pd['punctuation_marks'] = neg_train_data_pd['review'].apply(lambda x: sum([1 for x in x if x in punctuation_marks]))\n",
    "print('Average number of punctuation marks in review : ', neg_train_data_pd['punctuation_marks'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uoe6CYO8gH64"
   },
   "source": [
    "Unfortunately this perspective assumption was not justified too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o6ObsB2ogH65"
   },
   "source": [
    "#### Difference between positive and negative smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ShJZhAQcgH65"
   },
   "outputs": [],
   "source": [
    "def mood_counter(text):\n",
    "    braces = 0\n",
    "    \n",
    "    for i in text:\n",
    "    \n",
    "        if i == ')':\n",
    "            braces += 1\n",
    "        \n",
    "        elif i == '(':\n",
    "            braces -= 1\n",
    "    \n",
    "    return braces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GkGlEyzWgH66",
    "outputId": "d9f586b3-98fa-444a-8ca1-4156070df857"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average difference between positive and negative smiles in review :  0.02048\n"
     ]
    }
   ],
   "source": [
    "pos_train_data_pd['mood'] = pos_train_data_pd['review'].apply(lambda x: mood_counter(x))\n",
    "print('Average difference between positive and negative smiles in review : ', pos_train_data_pd['mood'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CwkyX8_ogH68",
    "outputId": "44fa38fc-0058-4337-9556-8bcaa567b352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average difference between positive and negative smiles in review :  0.04176\n"
     ]
    }
   ],
   "source": [
    "neg_train_data_pd['mood'] = neg_train_data_pd['review'].apply(lambda x: mood_counter(x))\n",
    "print('Average difference between positive and negative smiles in review : ', neg_train_data_pd['mood'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R69xvvLfgH7A"
   },
   "source": [
    "Values are to small but negative reviews twice more positive than positive reviews)) So we will not delete smile or sad brackets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yKbweDB-gH7B"
   },
   "source": [
    "#### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aTEoeeJ9gH7C",
    "outputId": "b2895057-8f72-4a84-a377-21df54f58d8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive review average sentiment :  0.19304342622068107\n",
      "negative review average sentiment :  0.013336767750285623\n"
     ]
    }
   ],
   "source": [
    "pos_train_data_pd['sentiment'] = pos_train_data_pd['review'].apply(lambda x: TextBlob(x).sentiment[0] )\n",
    "neg_train_data_pd['sentiment'] = neg_train_data_pd['review'].apply(lambda x: TextBlob(x).sentiment[0] )\n",
    "\n",
    "print('positive review average sentiment : ', pos_train_data_pd['sentiment'].mean())\n",
    "print('negative review average sentiment : ', neg_train_data_pd['sentiment'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GS0DiWJ2gH7E"
   },
   "source": [
    "There is notable difference between senses in these review categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UFApwKNhgH7F"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hlVVcCaUgH7F"
   },
   "source": [
    "#### Lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9z8NGhEegH7J"
   },
   "outputs": [],
   "source": [
    "pos_train_data_pd['review'] = pos_train_data_pd['review'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "neg_train_data_pd['review'] = neg_train_data_pd['review'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "pos_test_data_pd['review'] = pos_test_data_pd['review'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "neg_test_data_pd['review'] = neg_test_data_pd['review'].apply(lambda x: \" \".join(x.lower() for x in x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jSOZbuTcgH7L"
   },
   "source": [
    "#### Removing Punctuation\n",
    "\n",
    "Do not forget that we found some relation between class and brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GqX6iAOygH7M"
   },
   "outputs": [],
   "source": [
    "pos_train_data_pd['review'] = pos_train_data_pd['review'].str.replace('[^\\w\\s()]','')\n",
    "neg_train_data_pd['review'] = neg_train_data_pd['review'].str.replace('[^\\w\\s()]','')\n",
    "\n",
    "pos_test_data_pd['review'] = pos_test_data_pd['review'].str.replace('[^\\w\\s()]','')\n",
    "neg_test_data_pd['review'] = neg_test_data_pd['review'].str.replace('[^\\w\\s()]','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MqDzng_kgH7O"
   },
   "source": [
    "#### Removing common words\n",
    "\n",
    "Let's find commonly occuring words which may not be in stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-xVz8cdvgH7P",
    "outputId": "478f5c6a-220b-4d17-905d-83568ec636aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the      171441\n",
       "and       88009\n",
       "a         82810\n",
       "of        76495\n",
       "to        66315\n",
       "is        56974\n",
       "in        49445\n",
       "it        38213\n",
       "i         35251\n",
       "this      34650\n",
       "that      33946\n",
       "br        27474\n",
       "as        25782\n",
       "with      22981\n",
       "for       22131\n",
       "was       21817\n",
       "but       20300\n",
       "film      19468\n",
       "movie     18064\n",
       "his       17120\n",
       "on        16586\n",
       "are       14748\n",
       "he        14507\n",
       "you       14478\n",
       "not       13939\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_freq = pd.Series(' '.join(pos_train_data_pd['review']).split()).value_counts()\n",
    "pos_freq[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aJjbtCsogH7S",
    "outputId": "05c2859a-50fe-4dd3-e077-87e6005e24da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the      161787\n",
       "a         78498\n",
       "and       72601\n",
       "of        68650\n",
       "to        68579\n",
       "is        49763\n",
       "in        43094\n",
       "this      40321\n",
       "i         39703\n",
       "it        38401\n",
       "that      35131\n",
       "br        29576\n",
       "was       26146\n",
       "movie     23511\n",
       "for       21573\n",
       "but       21207\n",
       "with      20562\n",
       "as        20176\n",
       "film      17704\n",
       "on        16791\n",
       "not       15856\n",
       "have      15103\n",
       "you       14863\n",
       "are       14543\n",
       "be        14350\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_freq = pd.Series(' '.join(neg_train_data_pd['review']).split()).value_counts()\n",
    "neg_freq[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ceZYcXgBgH7V"
   },
   "source": [
    "First 25 words in both lists are similar and we can delete them. But what if not only first 25 are similar? There may be more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7pyXze3dgH7X",
    "outputId": "80c2f049-4b36-4edf-bddf-18af2bbe3857"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merge lists of 25 common words\n",
    "#common_words = list(pos_freq[:25].index) + list(neg_freq[:25].index)\n",
    "common_words = list(pos_freq[:150].index) + list(neg_freq[:150].index)\n",
    "#remove duplicate elements\n",
    "common_words = list(set(common_words))\n",
    "len(common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ykmMJMFlgH7a"
   },
   "source": [
    "Length 150 means that top-150 words in both categories are the same, length 300 means that top words are completely don't match. Length of the obtained merge list allow us to be sure that the most common words in both categories are almost the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1no_Kj04gH7a"
   },
   "source": [
    "Deleting the most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QCvxT5MsgH7b"
   },
   "outputs": [],
   "source": [
    "pos_train_data_pd['review'] = pos_train_data_pd['review'].apply(lambda x: \" \".join(x for x in x.split() if x not in common_words))\n",
    "neg_train_data_pd['review'] = neg_train_data_pd['review'].apply(lambda x: \" \".join(x for x in x.split() if x not in common_words))\n",
    "\n",
    "pos_test_data_pd['review'] = pos_test_data_pd['review'].apply(lambda x: \" \".join(x for x in x.split() if x not in common_words))\n",
    "neg_test_data_pd['review'] = neg_test_data_pd['review'].apply(lambda x: \" \".join(x for x in x.split() if x not in common_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m5k70EWWgH7c"
   },
   "source": [
    "#### Removing of Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "myknu-7vgH7d"
   },
   "outputs": [],
   "source": [
    "pos_train_data_pd['review'] = pos_train_data_pd['review'].apply(lambda x: \" \".join(x for x in x.split() if x not in cachedStopWords))\n",
    "neg_train_data_pd['review'] = neg_train_data_pd['review'].apply(lambda x: \" \".join(x for x in x.split() if x not in cachedStopWords))\n",
    "\n",
    "pos_test_data_pd['review'] = pos_test_data_pd['review'].apply(lambda x: \" \".join(x for x in x.split() if x not in cachedStopWords))\n",
    "neg_test_data_pd['review'] = neg_test_data_pd['review'].apply(lambda x: \" \".join(x for x in x.split() if x not in cachedStopWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X2q_mbtTgH7f"
   },
   "source": [
    "#### Removing HTML markup and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U4Z-3NjugH7h"
   },
   "outputs": [],
   "source": [
    "pos_train_data_pd['review'] = pos_train_data_pd['review'].apply(lambda x:  BeautifulSoup(x, 'html.parser').get_text())\n",
    "neg_train_data_pd['review'] = neg_train_data_pd['review'].apply(lambda x:  BeautifulSoup(x, 'html.parser').get_text())\n",
    "\n",
    "pos_test_data_pd['review'] = pos_test_data_pd['review'].apply(lambda x:  BeautifulSoup(x, 'html.parser').get_text())\n",
    "neg_test_data_pd['review'] = neg_test_data_pd['review'].apply(lambda x:  BeautifulSoup(x, 'html.parser').get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sgj01YJMgH7j"
   },
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-TUOtuigH7j"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    min_length = 3\n",
    "    words = map(lambda word: word.lower(), word_tokenize(text));\n",
    "   \n",
    "    tokens =(list(map(lambda token: PorterStemmer().stem(token), words)));\n",
    "    \n",
    "    #wn_lemmatizer = WordNetLemmatizer()\n",
    "    #tokens = [wn_lemmatizer.lemmatize(token) for token in words]\n",
    "    \n",
    "    p = re.compile('[a-zA-Z]+');\n",
    "    filtered_tokens = list(filter(lambda token:\n",
    "                  p.match(token) and len(token)>=min_length,\n",
    "         tokens));\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "def tf_idf(train_data, test_data=None, max_feats=50000):\n",
    "    tfidf = TfidfVectorizer(tokenizer=tokenize, \n",
    "                            min_df=3,\n",
    "                            max_df=0.90, \n",
    "                            max_features=max_feats,\n",
    "                            use_idf=True, \n",
    "                            sublinear_tf=True,\n",
    "                            norm='l2',\n",
    "                            ngram_range = (1,3));\n",
    "    \n",
    "    vectorised_train_data = tfidf.fit_transform(train_data)\n",
    "    if test_data==None:\n",
    "        return vectorised_train_data, tfidf\n",
    "    else:\n",
    "        vectorised_test_data = tfidf.transform(test_data)\n",
    "        return vectorised_train_data,vectorised_test_data, tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TbmkKnZHgH7l"
   },
   "source": [
    "#### Splitting the data\n",
    "\n",
    "The default split is 50/50 and now we wil make it 70/30. Don't forget to shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8H1EEsOGgH7m"
   },
   "outputs": [],
   "source": [
    "train_data = list(pos_train_data_pd['review']) + list(neg_train_data_pd['review'])\n",
    "test_data = list(pos_test_data_pd['review']) + list(neg_test_data_pd['review'])\n",
    "all_data = train_data + test_data\n",
    "\n",
    "train_labels = pos_train_labels + neg_train_labels\n",
    "test_labels = pos_test_labels + neg_test_labels\n",
    "all_labels = train_labels + test_labels\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(all_data, all_labels, test_size=0.3, \n",
    "                                                                    stratify=all_labels, shuffle=True,\n",
    "                                                                   random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ygo0SXlsgH7o"
   },
   "source": [
    "Let's binarize labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-bHiSQ_9gH7p"
   },
   "outputs": [],
   "source": [
    "train_labels = [1 if x in [7,8,9] else 0 for x in train_labels]\n",
    "test_labels = [1 if x in [7,8,9] else 0 for x in test_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R3WdGOLbgH7r"
   },
   "source": [
    "# Modeling and scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BAKiX-EhgH7r"
   },
   "source": [
    "Function that will fit our model and show auc roc score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7QCCWG65gH7s"
   },
   "outputs": [],
   "source": [
    "def clf_fit_show_metric(clf, X_train, y_train, X_test, y_test):\n",
    "    # Classifier\n",
    "    classifier = OneVsRestClassifier(clf)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    prob_pred = classifier.predict_proba(X_test)[:,1]\n",
    "    print('auc roc score : ', roc_auc_score(y_test, prob_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tkvAl_bdgH7u"
   },
   "source": [
    "## Vectorizing\n",
    "\n",
    "Let's start from 3000 features in tf-idf. Later we will check other values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_z66AclSgH7v",
    "outputId": "27441794-016a-4b96-d03e-025d531b6287"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are  35000  raws in the dataset\n",
      "there are  3000  features in the dataset\n",
      "Wall time: 3min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tokenisation\n",
    "vectorizer = TfidfVectorizer(stop_words=cachedStopWords,\n",
    "                             tokenizer=tokenize)\n",
    " \n",
    "# Learn and transform train documents\n",
    "vectorised_train_data, vectorised_test_data, tfidf = tf_idf(train_data, test_data, 3000)\n",
    "\n",
    "n = vectorised_train_data.shape[0] #how many raws we have in the dataset\n",
    "print('there are ', n, ' raws in the dataset')\n",
    "num_feats = vectorised_train_data.shape[1]\n",
    "print('there are ', num_feats, ' features in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F2_Vp2IzgH7x"
   },
   "outputs": [],
   "source": [
    "my_scorer = make_scorer(roc_auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Axv8YbigH70"
   },
   "source": [
    "# SGD classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3vLACdsPgH70"
   },
   "source": [
    "Now we are finding best parameters for SGD classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G8Fv3HydgH70",
    "outputId": "3716ff53-6944-423f-f55a-bf0f9045e4ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function :  log\n",
      "Alpha :  1e-05\n",
      "auc roc score :  0.813221186195\n",
      "\n",
      "\n",
      "Loss function :  log\n",
      "Alpha :  0.0001\n",
      "auc roc score :  0.821616130875\n",
      "\n",
      "\n",
      "Loss function :  log\n",
      "Alpha :  0.0005\n",
      "auc roc score :  0.814115456082\n",
      "\n",
      "\n",
      "Loss function :  log\n",
      "Alpha :  0.001\n",
      "auc roc score :  0.80902832317\n",
      "\n",
      "\n",
      "Loss function :  modified_huber\n",
      "Alpha :  1e-05\n",
      "auc roc score :  0.739094387847\n",
      "\n",
      "\n",
      "Loss function :  modified_huber\n",
      "Alpha :  0.0001\n",
      "auc roc score :  0.809693887246\n",
      "\n",
      "\n",
      "Loss function :  modified_huber\n",
      "Alpha :  0.0005\n",
      "auc roc score :  0.820085607962\n",
      "\n",
      "\n",
      "Loss function :  modified_huber\n",
      "Alpha :  0.001\n",
      "auc roc score :  0.821231410241\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Losf_values = ['log', 'modified_huber']\n",
    "Alpha_values = [1e-5, 1e-4, 5e-4, 1e-3]\n",
    "for Losf in Losf_values:  \n",
    "    for Alpha in Alpha_values:\n",
    "        print('Loss function : ', Losf)\n",
    "        print('Alpha : ', Alpha)\n",
    "        clf_fit_show_metric(SGDClassifier(loss=Losf, alpha=Alpha), \n",
    "                           vectorised_train_data, train_labels, \n",
    "                           vectorised_test_data, test_labels)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2XjobZiagH73"
   },
   "source": [
    "The best model have auc roc score near 0,82. Loss function is 'modified_huber' and regularization constant alpha is 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g_nLN8vVgH73"
   },
   "source": [
    "#### Finding best shape of tf-idf matrix\n",
    "\n",
    "Let's make an experiment in which we will find the best shape of tf-idf matrix on the example of the best SGD classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3HinbZZZgH74",
    "outputId": "23699c36-d873-4b4c-cb33-3e4b9df358aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num feats :  1000\n",
      "there are  35000  raws in the dataset\n",
      "there are  1000  features in the dataset\n",
      "auc roc score :  0.805717012513\n",
      "\n",
      "\n",
      "Num feats :  2000\n",
      "there are  35000  raws in the dataset\n",
      "there are  2000  features in the dataset\n",
      "auc roc score :  0.816463692356\n",
      "\n",
      "\n",
      "Num feats :  3000\n",
      "there are  35000  raws in the dataset\n",
      "there are  3000  features in the dataset\n",
      "auc roc score :  0.821276644379\n",
      "\n",
      "\n",
      "Num feats :  5000\n",
      "there are  35000  raws in the dataset\n",
      "there are  5000  features in the dataset\n",
      "auc roc score :  0.825726319557\n",
      "\n",
      "\n",
      "Num feats :  7000\n",
      "there are  35000  raws in the dataset\n",
      "there are  7000  features in the dataset\n",
      "auc roc score :  0.827892047925\n",
      "\n",
      "\n",
      "Num feats :  10000\n",
      "there are  35000  raws in the dataset\n",
      "there are  10000  features in the dataset\n",
      "auc roc score :  0.829479181601\n",
      "\n",
      "\n",
      "Num feats :  15000\n",
      "there are  35000  raws in the dataset\n",
      "there are  15000  features in the dataset\n",
      "auc roc score :  0.830917811542\n",
      "\n",
      "\n",
      "Num feats :  20000\n",
      "there are  35000  raws in the dataset\n",
      "there are  20000  features in the dataset\n",
      "auc roc score :  0.830975867954\n",
      "\n",
      "\n",
      "Wall time: 24min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_features = [1000, 2000, 3000, 5000, 7000, 10000, 15000, 20000]\n",
    "\n",
    "for feats in num_features:\n",
    "    print('Num feats : ', feats)\n",
    "    # Learn and transform train documents\n",
    "    vectorised_train_data, vectorised_test_data, tfidf = tf_idf(train_data, test_data, feats)\n",
    "\n",
    "    n = vectorised_train_data.shape[0] #how many raws we have in the dataset\n",
    "    print('there are ', n, ' raws in the dataset')\n",
    "    num_feats = vectorised_train_data.shape[1]\n",
    "    print('there are ', num_feats, ' features in the dataset')\n",
    "\n",
    "    clf_fit_show_metric(SGDClassifier(loss='modified_huber', alpha=0.001), \n",
    "                               vectorised_train_data, train_labels, \n",
    "                               vectorised_test_data, test_labels)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uGQziO59gH77"
   },
   "source": [
    "With an increase of number of features model becomes better, but after 10000 features profit is very small. So let's deal with 10000 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ziXRsCAmgH78"
   },
   "outputs": [],
   "source": [
    "# Learn and transform train documents\n",
    "vectorised_train_data, vectorised_test_data, tfidf = tf_idf(train_data, test_data, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1bvx3eHgH7-"
   },
   "source": [
    "# Support Vector Classification (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1pvkocexgH7-",
    "outputId": "38660f6c-8089-463b-fae7-eecd7897db75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc roc score :  0.825239733071\n",
      "Wall time: 58min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "clf_fit_show_metric( SVC(probability=True), \n",
    "                        vectorised_train_data, train_labels, \n",
    "                        vectorised_test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ycjd08rdgH8B"
   },
   "source": [
    "Computing of this methon is very long and result are pretty the same as SGD model gave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xe1ga7pzgH8C"
   },
   "source": [
    "# Naive Bayes classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ig8qiC5PgH8C",
    "outputId": "1b962cf8-9f0e-4f9d-9560-641857da2853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB function :  <class 'sklearn.naive_bayes.BernoulliNB'>\n",
      "auc roc score :  0.775429017718\n",
      "\n",
      "\n",
      "NB function :  <class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "auc roc score :  0.808277821996\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NB_functions = [BernoulliNB, MultinomialNB]\n",
    "for NB_func in NB_functions:\n",
    "    print('NB function : ', NB_func)\n",
    "    clf_fit_show_metric(NB_func(), \n",
    "                        vectorised_train_data, train_labels, \n",
    "                        vectorised_test_data, test_labels)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "81Z7uGLWgH8E"
   },
   "source": [
    "Multinomial Naive Bayes have auc roc score about 0.81, that is a little worse than metric of the SGD model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DsEZcgYdgH8E"
   },
   "source": [
    "# TruncatedSVD\n",
    "\n",
    "Now let's try to reduce dimension of tf-idf sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I37yiD4PgH8F"
   },
   "outputs": [],
   "source": [
    "tsvd = TruncatedSVD(n_components = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgP0cTqjgH8G"
   },
   "source": [
    "Let's make a pipeline to make data preprocessing easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YopvFBXxgH8G"
   },
   "outputs": [],
   "source": [
    "preprocessing = Pipeline(steps = [('tfidf', tfidf), ('tsvd',tsvd)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QZ79I1bngH8J"
   },
   "source": [
    "We will estimate efficiency of the decomposition on the example of SGD classifier with the best parameters we found above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-DmiilwkgH8K",
    "outputId": "28872446-1c9d-4f0d-bc69-ce0bf3c1cb1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc roc score :  0.821033932538\n",
      "Wall time: 3min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sgd = SGDClassifier(loss='modified_huber', alpha=0.001)\n",
    "bst_clf_pipe = Pipeline(steps=[('preprocessing', preprocessing),\n",
    "                           ('sgd', sgd)\n",
    "                    ])\n",
    "\n",
    "clf_fit_show_metric(bst_clf_pipe, \n",
    "                               train_data, train_labels, \n",
    "                               test_data, test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M934rTUcgH8N"
   },
   "source": [
    "Reducing the dimension of the matrix didn't make a profit but take more time than fitting without SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5njR6jNgH8O"
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fUkc3UfzgH8O"
   },
   "outputs": [],
   "source": [
    "TextExpl = TextExplainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ph2CjEDgH8P"
   },
   "source": [
    "#### Let's find some presentative positive text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OWsYdlSNgH8P",
    "outputId": "cdea9b5b-cb76-4185-9811-65730ef9239f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real class :  0\n",
      "Predicted class :  [1]\n",
      "Probabilities :  [[0.01444528 0.98555472]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,15000):\n",
    "    pos_text = test_data[i]\n",
    "    if(bst_clf_pipe.predict_proba([pos_text])[:,1]>0.97):\n",
    "        positive_index = i\n",
    "        break\n",
    "\n",
    "pos_text = test_data[positive_index]\n",
    "\n",
    "print('Real class : ' , test_labels[positive_index])\n",
    "print('Predicted class : ', bst_clf_pipe.predict([pos_text]))\n",
    "print('Probabilities : ', bst_clf_pipe.predict_proba([pos_text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ic7dAnqWgH8T"
   },
   "outputs": [],
   "source": [
    "feature_names_tfidf = bst_clf_pipe.named_steps['preprocessing'].named_steps['tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s0FtePdugH8X",
    "outputId": "16faccc3-c45d-4142-f58a-312231ef9aec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=aaron\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.971</b>, score <b>3.496</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +3.649\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "            <tr style=\"background-color: hsl(0, 100.00%, 97.83%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        -0.153\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"opacity: 0.80\">checked impulse browsing </span><span style=\"background-color: hsl(0, 100.00%, 92.82%); opacity: 0.82\" title=\"-0.082\">store</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.05%); opacity: 0.82\" title=\"-0.095\">couldnt</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 71.53%); opacity: 0.92\" title=\"0.589\">pleasantly</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 65.24%); opacity: 0.96\" title=\"0.784\">surprised</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.41%); opacity: 0.82\" title=\"-0.089\">mom</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.32%); opacity: 0.83\" title=\"-0.145\">watched</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.78%); opacity: 0.81\" title=\"0.039\">together</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.22%); opacity: 0.82\" title=\"-0.092\">thoroughly</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 72.79%); opacity: 0.92\" title=\"0.553\">enjoyed</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.75%); opacity: 0.81\" title=\"-0.027\">isnt</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.61%); opacity: 0.83\" title=\"0.121\">typical</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.51%); opacity: 0.81\" title=\"-0.042\">chickflick</span><span style=\"opacity: 0.80\"> sappy </span><span style=\"background-color: hsl(0, 100.00%, 83.23%); opacity: 0.86\" title=\"-0.277\">tears</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.16%); opacity: 0.84\" title=\"0.210\">definitely</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.98%); opacity: 0.83\" title=\"0.133\">touches</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.51%); opacity: 0.84\" title=\"-0.203\">nerve</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.75%); opacity: 0.85\" title=\"0.219\">twist</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.31%); opacity: 0.82\" title=\"0.108\">ending</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.33%); opacity: 0.85\" title=\"0.229\">although</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.22%); opacity: 0.83\" title=\"0.147\">unexpected</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.62%); opacity: 0.85\" title=\"0.245\">tragic</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 79.32%); opacity: 0.88\" title=\"0.373\">overall</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.12%); opacity: 0.82\" title=\"0.112\">effect</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.81%); opacity: 0.82\" title=\"-0.099\">harmed</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 83.21%); opacity: 0.86\" title=\"-0.277\">reese</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.72%); opacity: 0.84\" title=\"-0.177\">witherspoon</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.83%); opacity: 0.83\" title=\"-0.155\">actress</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.81%); opacity: 0.83\" title=\"0.155\">debut</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 63.73%); opacity: 0.97\" title=\"0.833\">definitely</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 60.00%); opacity: 1.00\" title=\"0.958\">worth</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.78%); opacity: 0.82\" title=\"-0.100\">recognize</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.62%); opacity: 0.81\" title=\"0.041\">supporting</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 91.52%); opacity: 0.82\" title=\"0.105\">play</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.84%); opacity: 0.83\" title=\"-0.155\">important</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.64%); opacity: 0.81\" title=\"-0.054\">supporting</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.28%); opacity: 0.82\" title=\"0.091\">roles</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.87%); opacity: 0.81\" title=\"-0.037\">moon</span><span style=\"opacity: 0.80\"> believable </span><span style=\"background-color: hsl(120, 100.00%, 99.33%); opacity: 0.80\" title=\"0.003\">teenager</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.83%); opacity: 0.81\" title=\"-0.038\">falling</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.91%); opacity: 0.83\" title=\"-0.134\">women</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.52%); opacity: 0.85\" title=\"0.224\">definitely</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 85.57%); opacity: 0.85\" title=\"0.223\">relate</span><span style=\"opacity: 0.80\"> everythingfrom </span><span style=\"background-color: hsl(0, 100.00%, 95.90%); opacity: 0.81\" title=\"-0.037\">witherspoons</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.73%); opacity: 0.83\" title=\"-0.157\">words</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 81.82%); opacity: 0.86\" title=\"0.311\">subtle</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.14%); opacity: 0.81\" title=\"-0.034\">glances</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.84%); opacity: 0.85\" title=\"0.240\">subtle</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.51%); opacity: 0.83\" title=\"-0.142\">emotions</span><span style=\"opacity: 0.80\"> (</span><span style=\"background-color: hsl(0, 100.00%, 95.04%); opacity: 0.81\" title=\"-0.049\">raging</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.23%); opacity: 0.82\" title=\"0.076\">typical</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.66%); opacity: 0.81\" title=\"0.028\">teenage</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.74%); opacity: 0.81\" title=\"-0.068\">girl</span><span style=\"opacity: 0.80\">) </span><span style=\"background-color: hsl(120, 100.00%, 86.77%); opacity: 0.84\" title=\"0.197\">shes</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.66%); opacity: 0.84\" title=\"0.178\">playing</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.22%); opacity: 0.83\" title=\"-0.128\">confused</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.37%); opacity: 0.81\" title=\"-0.058\">come</span><span style=\"opacity: 0.80\"> across </span><span style=\"background-color: hsl(0, 100.00%, 88.88%); opacity: 0.83\" title=\"-0.154\">silly</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.55%); opacity: 0.83\" title=\"-0.141\">immature</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 81.54%); opacity: 0.87\" title=\"0.317\">appreciated</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.06%); opacity: 0.82\" title=\"0.078\">considering</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 81.76%); opacity: 0.87\" title=\"0.312\">today</span>\n",
       "    </p>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TextExpl.fit(pos_text, bst_clf_pipe.predict_proba)\n",
    "display(TextExpl.show_prediction(target_names=feature_names_tfidf.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MMsJfOLvgH8Z"
   },
   "source": [
    "Here we can see such words as pleasantly, surprised (and they both together because we 2-grams), enjoyed, worth (with definitely). These words are charachterize this review as positive with probability of 0.97"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5jfOs7DkgH8a"
   },
   "source": [
    "#### Let's find some negative text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uRBaEhL8gH8a",
    "outputId": "f688e1a7-3f57-4945-ceae-e514a338b3f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real class :  0\n",
      "Predicted class :  [0]\n",
      "Probabilities :  [[1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,15000):\n",
    "    neg_text = test_data[i]\n",
    "    if(bst_clf_pipe.predict_proba([neg_text])[:,0]>0.97):\n",
    "        negative_index = i\n",
    "        break\n",
    "\n",
    "neg_text = test_data[negative_index]\n",
    "\n",
    "print('Real class : ' , test_labels[negative_index])\n",
    "print('Predicted class : ', bst_clf_pipe.predict([neg_text]))\n",
    "print('Probabilities : ', bst_clf_pipe.predict_proba([neg_text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "REzRBZc6gH8c",
    "outputId": "1072ef8f-cf2b-4d0d-8870-533588a924d1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    table.eli5-weights tr:hover {\n",
       "        filter: brightness(85%);\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "        \n",
       "\n",
       "    \n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "    \n",
       "        \n",
       "        \n",
       "    \n",
       "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
       "            <b>\n",
       "    \n",
       "        y=aamir\n",
       "    \n",
       "</b>\n",
       "\n",
       "    \n",
       "    (probability <b>0.996</b>, score <b>-5.599</b>)\n",
       "\n",
       "top features\n",
       "        </p>\n",
       "    \n",
       "    <table class=\"eli5-weights\"\n",
       "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
       "        <thead>\n",
       "        <tr style=\"border: none;\">\n",
       "            \n",
       "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
       "                    Contribution<sup>?</sup>\n",
       "                </th>\n",
       "            \n",
       "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
       "            \n",
       "        </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +5.258\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        Highlighted in text (sum)\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "            <tr style=\"background-color: hsl(120, 100.00%, 97.05%); border: none;\">\n",
       "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
       "        +0.342\n",
       "    </td>\n",
       "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
       "        &lt;BIAS&gt;\n",
       "    </td>\n",
       "    \n",
       "</tr>\n",
       "        \n",
       "        \n",
       "\n",
       "        \n",
       "        \n",
       "\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n",
       "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
       "        <span style=\"background-color: hsl(120, 100.00%, 85.63%); opacity: 0.85\" title=\"0.207\">saw</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 79.53%); opacity: 0.88\" title=\"0.343\">disaster</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 78.00%); opacity: 0.89\" title=\"0.380\">talking</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.22%); opacity: 0.82\" title=\"-0.102\">volcano</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 70.18%); opacity: 0.93\" title=\"-0.587\">lot</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 78.20%); opacity: 0.88\" title=\"-0.376\">certainly</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 78.48%); opacity: 0.88\" title=\"0.369\">care</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.34%); opacity: 0.83\" title=\"-0.117\">fact</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.73%); opacity: 0.83\" title=\"-0.146\">volcano</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.14%); opacity: 0.81\" title=\"-0.044\">erupting</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 78.63%); opacity: 0.88\" title=\"-0.365\">underneath</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 82.18%); opacity: 0.86\" title=\"-0.281\">downtown</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.55%); opacity: 0.82\" title=\"0.081\">la</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 81.77%); opacity: 0.87\" title=\"0.291\">possible</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.29%); opacity: 0.83\" title=\"0.155\">perhaps</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.10%); opacity: 0.81\" title=\"0.044\">isnt</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.64%); opacity: 0.82\" title=\"-0.095\">sure</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.49%); opacity: 0.81\" title=\"-0.039\">isnt</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 95.59%); opacity: 0.81\" title=\"0.038\">ill</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.66%); opacity: 0.85\" title=\"0.227\">explain</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.49%); opacity: 0.83\" title=\"0.115\">whybr</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 95.87%); opacity: 0.81\" title=\"-0.035\">lava</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 80.96%); opacity: 0.87\" title=\"-0.309\">flows</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.48%); opacity: 0.85\" title=\"-0.210\">average</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.50%); opacity: 0.83\" title=\"-0.151\">volcano</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.95%); opacity: 0.82\" title=\"-0.107\">volcano</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.87%); opacity: 0.83\" title=\"-0.126\">vesuvius</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.77%); opacity: 0.82\" title=\"-0.093\">etna</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.51%); opacity: 0.81\" title=\"-0.066\">mount</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.32%); opacity: 0.84\" title=\"-0.193\">pinatubo</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.56%); opacity: 0.82\" title=\"-0.081\">together</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 81.45%); opacity: 0.87\" title=\"0.298\">look</span><span style=\"opacity: 0.80\"> barbecue </span><span style=\"background-color: hsl(120, 100.00%, 93.73%); opacity: 0.81\" title=\"0.063\">lava</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 86.79%); opacity: 0.84\" title=\"-0.184\">flowing</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.59%); opacity: 0.82\" title=\"-0.096\">volcano</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.52%); opacity: 0.83\" title=\"-0.150\">sure</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.57%); opacity: 0.84\" title=\"0.168\">director</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 71.28%); opacity: 0.92\" title=\"-0.557\">lot</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.82%); opacity: 0.84\" title=\"0.183\">money</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.89%); opacity: 0.84\" title=\"0.162\">spend</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.85%); opacity: 0.81\" title=\"0.024\">wonder</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 96.12%); opacity: 0.81\" title=\"-0.032\">spent</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 81.90%); opacity: 0.86\" title=\"0.288\">special</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.11%); opacity: 0.84\" title=\"-0.158\">effects</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.10%); opacity: 0.85\" title=\"0.239\">script</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.27%); opacity: 0.82\" title=\"0.070\">saying</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 96.09%); opacity: 0.81\" title=\"0.032\">hired</span><span style=\"opacity: 0.80\"> top </span><span style=\"background-color: hsl(0, 100.00%, 99.63%); opacity: 0.80\" title=\"-0.001\">cast</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.22%); opacity: 0.84\" title=\"-0.175\">opposite</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 89.05%); opacity: 0.83\" title=\"0.140\">call</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 77.18%); opacity: 0.89\" title=\"-0.401\">performances</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 74.91%); opacity: 0.90\" title=\"0.459\">unbelievably</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 60.00%); opacity: 1.00\" title=\"0.894\">poor</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 76.92%); opacity: 0.89\" title=\"0.407\">entire</span><span style=\"opacity: 0.80\"> worsebr </span><span style=\"background-color: hsl(120, 100.00%, 79.56%); opacity: 0.88\" title=\"0.342\">whats</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.55%); opacity: 0.85\" title=\"0.230\">wrong</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 84.10%); opacity: 0.85\" title=\"0.239\">script</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.19%); opacity: 0.83\" title=\"0.120\">probably</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.22%); opacity: 0.83\" title=\"0.156\">ask</span><span style=\"opacity: 0.80\"> tell </span><span style=\"background-color: hsl(120, 100.00%, 93.58%); opacity: 0.81\" title=\"0.066\">comes</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 81.37%); opacity: 0.87\" title=\"0.300\">idea</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 94.19%); opacity: 0.81\" title=\"-0.057\">standing</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.97%); opacity: 0.81\" title=\"-0.060\">yards</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 87.54%); opacity: 0.84\" title=\"0.169\">feet</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.73%); opacity: 0.81\" title=\"0.063\">lava</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.19%); opacity: 0.83\" title=\"-0.138\">without</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 84.42%); opacity: 0.85\" title=\"-0.232\">getting</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 92.12%); opacity: 0.82\" title=\"0.088\">burned</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 88.69%); opacity: 0.83\" title=\"-0.147\">hide</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 82.99%); opacity: 0.86\" title=\"-0.263\">heath</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 98.64%); opacity: 0.80\" title=\"0.007\">sinking</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 88.59%); opacity: 0.83\" title=\"0.149\">earth</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 87.85%); opacity: 0.84\" title=\"-0.163\">flow</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 93.73%); opacity: 0.81\" title=\"0.063\">lava</span><span style=\"opacity: 0.80\"> isnt </span><span style=\"background-color: hsl(120, 100.00%, 86.63%); opacity: 0.84\" title=\"0.187\">foot</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.72%); opacity: 0.82\" title=\"-0.111\">high</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.43%); opacity: 0.85\" title=\"-0.211\">sure</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.08%); opacity: 0.81\" title=\"0.058\">wouldnt</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 90.41%); opacity: 0.83\" title=\"-0.116\">proud</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 69.32%); opacity: 0.94\" title=\"0.612\">wrote</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 71.28%); opacity: 0.92\" title=\"0.557\">script</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 76.54%); opacity: 0.89\" title=\"0.417\">apparently</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.17%); opacity: 0.84\" title=\"0.196\">script</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 94.45%); opacity: 0.81\" title=\"0.053\">writers</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 89.67%); opacity: 0.83\" title=\"-0.129\">hollywood</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 92.24%); opacity: 0.82\" title=\"-0.086\">mind</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.96%); opacity: 0.81\" title=\"-0.060\">believability</span><span style=\"opacity: 0.80\"> long </span><span style=\"background-color: hsl(120, 100.00%, 87.73%); opacity: 0.84\" title=\"0.165\">pays</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 71.88%); opacity: 0.92\" title=\"0.540\">money</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.40%); opacity: 0.82\" title=\"-0.068\">moneybr</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 85.00%); opacity: 0.85\" title=\"-0.220\">youll</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 90.26%); opacity: 0.83\" title=\"0.119\">probably</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.63%); opacity: 0.81\" title=\"-0.065\">agree</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 93.63%); opacity: 0.81\" title=\"-0.065\">hollywoods</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(120, 100.00%, 86.06%); opacity: 0.84\" title=\"0.198\">disaster</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 70.98%); opacity: 0.93\" title=\"-0.565\">worth</span><span style=\"opacity: 0.80\"> </span><span style=\"background-color: hsl(0, 100.00%, 91.47%); opacity: 0.82\" title=\"-0.098\">310</span>\n",
       "    </p>\n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "    \n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TextExpl.fit(neg_text, bst_clf_pipe.predict_proba)\n",
    "display(TextExpl.show_prediction(target_names=feature_names_tfidf.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o2UwUSDQgH8e"
   },
   "source": [
    "And here we see words disaster, poor, unbelievably. Interesting that there is a lot of speculation about the script of the movie. So this review is negative with great chance"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW4_edited.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
