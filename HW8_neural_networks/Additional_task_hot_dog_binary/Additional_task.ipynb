{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TKYq-dPXpADf"
   },
   "source": [
    "Code is taken from https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html article. Added test generator with metrics evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KM-aF0IVpADh"
   },
   "source": [
    "# Loading needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gztDFFjopADi",
    "outputId": "312b61cd-05d8-4d16-808e-c134d5eebb74"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OyzwuN03pADs"
   },
   "source": [
    "Dataset was splitted manually, so there are 336 train, 80 validation and 80 test images of each class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uDrabkEfpADt"
   },
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = './data/train'\n",
    "validation_data_dir = './data/val'\n",
    "test_data_dir = './data/test'\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cfEXLV1mpAD1",
    "outputId": "cbd35807-ac29-4cf9-da4b-38acc7b25936"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 672 images belonging to 2 classes.\n",
      "Found 160 images belonging to 2 classes.\n",
      "Found 160 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "augmentation_datagen = ImageDataGenerator(rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   rescale=1. / 255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "train_generator = augmentation_datagen.flow_from_directory(train_data_dir,\n",
    "                                                    target_size=(img_width,\n",
    "                                                                 img_height),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='binary')\n",
    "\n",
    "# this is the augmentation configuration we will use for\n",
    "# testing and validation: only rescaling\n",
    "rescale_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_generator = rescale_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "test_generator = rescale_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ogoN1FP0pADr"
   },
   "source": [
    "# Training a small convnet from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opD3i4AcpADx"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ML7sbZgmpAD6",
    "outputId": "0e41eb04-a8df-4ab9-d17b-a4454199021a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "42/42 [==============================] - 12s 261ms/step - loss: 1.0254 - accuracy: 0.4816 - val_loss: 0.7356 - val_accuracy: 0.5000\n",
      "Epoch 2/100\n",
      "42/42 [==============================] - 10s 246ms/step - loss: 0.6947 - accuracy: 0.5591 - val_loss: 0.6924 - val_accuracy: 0.5250\n",
      "Epoch 3/100\n",
      "42/42 [==============================] - 12s 276ms/step - loss: 0.6961 - accuracy: 0.5005 - val_loss: 0.6928 - val_accuracy: 0.5813\n",
      "Epoch 4/100\n",
      "42/42 [==============================] - 11s 267ms/step - loss: 0.7034 - accuracy: 0.5099 - val_loss: 0.6927 - val_accuracy: 0.5250\n",
      "Epoch 5/100\n",
      "42/42 [==============================] - 10s 242ms/step - loss: 0.7224 - accuracy: 0.5856 - val_loss: 0.6928 - val_accuracy: 0.5000\n",
      "Epoch 6/100\n",
      "42/42 [==============================] - 10s 244ms/step - loss: 0.6910 - accuracy: 0.5630 - val_loss: 0.7246 - val_accuracy: 0.5000\n",
      "Epoch 7/100\n",
      "42/42 [==============================] - 10s 240ms/step - loss: 0.6869 - accuracy: 0.5892 - val_loss: 0.6917 - val_accuracy: 0.5000\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x229ddd63a20>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_accuracy',\n",
    "                          min_delta=0.01,\n",
    "                          patience=4,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True,\n",
    "                          mode='max')\n",
    "\n",
    "# Defining checkpoint callback\n",
    "filepath = './baseline.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath,\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=0,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "\n",
    "model.fit(train_generator,\n",
    "          epochs=epochs,\n",
    "          validation_data=validation_generator,\n",
    "          callbacks=[earlystop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PD_wGaA8pAD-",
    "outputId": "3f0a7553-3bcb-4afd-a2ea-306febacbb85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 2s 158ms/step - loss: 0.6931 - accuracy: 0.4875\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.693109</td>\n",
       "      <td>0.4875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy\n",
       "0  0.693109    0.4875"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([model.evaluate(test_generator)],\n",
    "                 columns=model.metrics_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fXfMdUz_pAED"
   },
   "source": [
    "# Using the bottleneck features of a pre-trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hNxz66zHpAEE"
   },
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "\n",
    "nb_train_samples = 336\n",
    "nb_validation_samples = 80\n",
    "nb_test_samples = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j89Cu5J7pAEJ"
   },
   "outputs": [],
   "source": [
    "def save_bottlebeck_features():\n",
    "\n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "    generator = rescale_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None, # this means our generator will only yield batches of data, no labels\n",
    "        shuffle=False)# our data will be in order, so all first 1000 images will be cats, then 1000 dogs\n",
    "    \n",
    "    # the predict method returns the output of a model, given\n",
    "    # a generator that yields batches of numpy data\n",
    "    bottleneck_features_train = model.predict(\n",
    "        generator, nb_train_samples // batch_size)\n",
    "    np.save('bottleneck_features_train.npy', bottleneck_features_train)\n",
    "\n",
    "    generator = rescale_datagen.flow_from_directory(validation_data_dir,\n",
    "                                                    target_size=(img_width,\n",
    "                                                                 img_height),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode=None,\n",
    "                                                    shuffle=False)\n",
    "\n",
    "    bottleneck_features_validation = model.predict(\n",
    "        generator, nb_validation_samples // batch_size)\n",
    "    np.save('bottleneck_features_validation.npy',\n",
    "            bottleneck_features_validation)\n",
    "\n",
    "    generator = rescale_datagen.flow_from_directory(test_data_dir,\n",
    "                                                    target_size=(img_width,\n",
    "                                                                 img_height),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode=None,\n",
    "                                                    shuffle=False)\n",
    "\n",
    "    bottleneck_features_validation = model.predict(\n",
    "        generator, nb_validation_samples // batch_size)\n",
    "    np.save('bottleneck_features_test.npy', bottleneck_features_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EUZmIUpBpAEM"
   },
   "outputs": [],
   "source": [
    "def train_top_model():\n",
    "    train_data = np.load('bottleneck_features_train.npy')\n",
    "    train_labels = np.array([0] * (nb_train_samples) + [1] *\n",
    "                            (nb_train_samples))\n",
    "\n",
    "    validation_data = np.load('bottleneck_features_validation.npy')\n",
    "    validation_labels = np.array([0] * (nb_validation_samples) + [1] *\n",
    "                                 (nb_validation_samples))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Defining early stopping callback\n",
    "    earlystop = EarlyStopping(monitor='val_accuracy',\n",
    "                              min_delta=0.01,\n",
    "                              patience=8,\n",
    "                              verbose=1,\n",
    "                              restore_best_weights=True,\n",
    "                              mode='max')\n",
    "\n",
    "    # Defining checkpoint callback\n",
    "    filepath = './bottleneck_baseline.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath,\n",
    "                                 monitor='val_accuracy',\n",
    "                                 verbose=0,\n",
    "                                 save_best_only=True,\n",
    "                                 mode='max')\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    model.fit(train_data,\n",
    "              train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels),\n",
    "              callbacks=[earlystop, checkpoint])\n",
    "    model.save_weights(top_model_weights_path)\n",
    "\n",
    "    test_data = np.load('bottleneck_features_test.npy')\n",
    "    test_labels = np.array([0] * (nb_test_samples) + [1] *\n",
    "                           (nb_test_samples))\n",
    "\n",
    "    df = pd.DataFrame([model.evaluate(test_data, test_labels)],\n",
    "                      columns=model.metrics_names)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9rq_gO1gpAEQ",
    "outputId": "3a62fc99-a1bb-47f2-d2e6-bd168920755d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 2,097,665\n",
      "Trainable params: 2,097,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 1.9676 - accuracy: 0.5756 - val_loss: 0.4580 - val_accuracy: 0.7688\n",
      "Epoch 2/100\n",
      "42/42 [==============================] - 1s 20ms/step - loss: 0.6404 - accuracy: 0.7182 - val_loss: 0.6314 - val_accuracy: 0.6875\n",
      "Epoch 3/100\n",
      "42/42 [==============================] - 1s 21ms/step - loss: 0.3951 - accuracy: 0.8264 - val_loss: 0.6785 - val_accuracy: 0.7250\n",
      "Epoch 4/100\n",
      "42/42 [==============================] - 1s 22ms/step - loss: 0.4186 - accuracy: 0.8262 - val_loss: 0.5949 - val_accuracy: 0.7375\n",
      "Epoch 5/100\n",
      "42/42 [==============================] - 1s 21ms/step - loss: 0.3691 - accuracy: 0.8611 - val_loss: 0.6382 - val_accuracy: 0.7625\n",
      "Epoch 6/100\n",
      "42/42 [==============================] - 1s 20ms/step - loss: 0.2668 - accuracy: 0.8934 - val_loss: 0.8058 - val_accuracy: 0.7437\n",
      "Epoch 7/100\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 0.3368 - accuracy: 0.8788 - val_loss: 0.7428 - val_accuracy: 0.7875\n",
      "Epoch 8/100\n",
      "42/42 [==============================] - 1s 21ms/step - loss: 0.1925 - accuracy: 0.9254 - val_loss: 0.5644 - val_accuracy: 0.7625\n",
      "Epoch 9/100\n",
      "42/42 [==============================] - 1s 21ms/step - loss: 0.1731 - accuracy: 0.9238 - val_loss: 0.5998 - val_accuracy: 0.7688\n",
      "Epoch 10/100\n",
      "42/42 [==============================] - 1s 20ms/step - loss: 0.1907 - accuracy: 0.9242 - val_loss: 0.5496 - val_accuracy: 0.7812\n",
      "Epoch 11/100\n",
      "42/42 [==============================] - 1s 21ms/step - loss: 0.1119 - accuracy: 0.9572 - val_loss: 0.6052 - val_accuracy: 0.8062\n",
      "Epoch 12/100\n",
      "42/42 [==============================] - 1s 21ms/step - loss: 0.1204 - accuracy: 0.9533 - val_loss: 0.5321 - val_accuracy: 0.7688\n",
      "Epoch 13/100\n",
      "42/42 [==============================] - 1s 21ms/step - loss: 0.0967 - accuracy: 0.9667 - val_loss: 0.8057 - val_accuracy: 0.7437\n",
      "Epoch 14/100\n",
      "42/42 [==============================] - 1s 20ms/step - loss: 0.0684 - accuracy: 0.9774 - val_loss: 0.6844 - val_accuracy: 0.7688\n",
      "Epoch 15/100\n",
      "42/42 [==============================] - 1s 20ms/step - loss: 0.0435 - accuracy: 0.9826 - val_loss: 0.8715 - val_accuracy: 0.7625\n",
      "Epoch 16/100\n",
      "42/42 [==============================] - 1s 20ms/step - loss: 0.0913 - accuracy: 0.9706 - val_loss: 0.7687 - val_accuracy: 0.7812\n",
      "Epoch 17/100\n",
      "42/42 [==============================] - 1s 21ms/step - loss: 0.0340 - accuracy: 0.9928 - val_loss: 0.7965 - val_accuracy: 0.7500\n",
      "Epoch 18/100\n",
      "42/42 [==============================] - 1s 23ms/step - loss: 0.0240 - accuracy: 0.9941 - val_loss: 0.8943 - val_accuracy: 0.7688\n",
      "Epoch 19/100\n",
      "42/42 [==============================] - 1s 21ms/step - loss: 0.0413 - accuracy: 0.9777 - val_loss: 1.0045 - val_accuracy: 0.7437\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00019: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.8406 - accuracy: 0.7875\n"
     ]
    }
   ],
   "source": [
    "save_bottlebeck_features()\n",
    "metrics = train_top_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5nL0Y0TbpAEU",
    "outputId": "7aa7b7fe-9047-4113-fb35-d1fff7060213",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.840628</td>\n",
       "      <td>0.7875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy\n",
       "0  0.840628    0.7875"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6gzoBD-KpAEa"
   },
   "source": [
    "# Fine-tuning the top layers of a a pre-trained network\n",
    "To further improve our previous result, we can try to \"fine-tune\" the last convolutional block of the VGG16 model alongside the top-level classifier. Fine-tuning consist in starting from a trained network, then re-training it on a new dataset using very small weight updates. In our case, this can be done in 3 steps:\n",
    "1. instantiate the convolutional base of VGG16 and load its weights\n",
    "2. add our previously defined fully-connected model on top, and load its weights\n",
    "3. freeze the layers of the VGG16 model up to the last convolutional block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Py1Sf5KapAEd"
   },
   "outputs": [],
   "source": [
    "# path to the model weights files.\n",
    "weights_path = '../keras/examples/vgg16_weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90kp38tCpAEg",
    "outputId": "c55e2ddc-c2d6-47b9-d69b-594d7847a6b4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "sequential_8 (Sequential)    (None, 1)                 2097665   \n",
      "=================================================================\n",
      "Total params: 16,812,353\n",
      "Trainable params: 9,177,089\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the VGG16 network\n",
    "base_model = applications.VGG16(weights='imagenet',\n",
    "                                include_top=False,\n",
    "                                input_shape=(150, 150, 3))\n",
    "print('Model loaded.')\n",
    "\n",
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "# add the model on top of the convolutional base\n",
    "model = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n",
    "\n",
    "# set the first 15 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "# fine-tuning should be done with a very slow learning rate, and typically \n",
    "# with the SGD optimizer rather than an adaptative learning rate optimizer \n",
    "# such as RMSProp. This is to make sure that the magnitude of the updates \n",
    "# stays very small, so as not to wreck the previously learned features.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q9obs2CSpAEm",
    "outputId": "09314851-3f10-4d9d-c4c6-4fadc277c761",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "42/42 [==============================] - 65s 2s/step - loss: 0.4998 - accuracy: 0.7980 - val_loss: 0.5365 - val_accuracy: 0.7937\n",
      "Epoch 2/100\n",
      "42/42 [==============================] - 62s 1s/step - loss: 0.3973 - accuracy: 0.8299 - val_loss: 0.4092 - val_accuracy: 0.8188\n",
      "Epoch 3/100\n",
      "42/42 [==============================] - 62s 1s/step - loss: 0.3985 - accuracy: 0.8177 - val_loss: 0.4704 - val_accuracy: 0.7937\n",
      "Epoch 4/100\n",
      "42/42 [==============================] - 62s 1s/step - loss: 0.4048 - accuracy: 0.8412 - val_loss: 0.4143 - val_accuracy: 0.8188\n",
      "Epoch 5/100\n",
      "42/42 [==============================] - 62s 1s/step - loss: 0.3622 - accuracy: 0.8395 - val_loss: 0.3892 - val_accuracy: 0.8250\n",
      "Epoch 6/100\n",
      "42/42 [==============================] - 62s 1s/step - loss: 0.3520 - accuracy: 0.8423 - val_loss: 0.3942 - val_accuracy: 0.8313\n",
      "Epoch 7/100\n",
      "42/42 [==============================] - 62s 1s/step - loss: 0.2924 - accuracy: 0.8824 - val_loss: 0.3871 - val_accuracy: 0.8438\n",
      "Epoch 8/100\n",
      "42/42 [==============================] - 61s 1s/step - loss: 0.3142 - accuracy: 0.8521 - val_loss: 0.3687 - val_accuracy: 0.8250\n",
      "Epoch 9/100\n",
      "42/42 [==============================] - 65s 2s/step - loss: 0.2212 - accuracy: 0.9099 - val_loss: 0.4071 - val_accuracy: 0.8313\n",
      "Epoch 10/100\n",
      "42/42 [==============================] - 62s 1s/step - loss: 0.2393 - accuracy: 0.8948 - val_loss: 0.3860 - val_accuracy: 0.8250\n",
      "Epoch 11/100\n",
      "42/42 [==============================] - 62s 1s/step - loss: 0.2454 - accuracy: 0.9033 - val_loss: 0.3923 - val_accuracy: 0.8500\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x229dcb05eb8>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining early stopping callback\n",
    "earlystop = EarlyStopping(monitor='val_accuracy',\n",
    "                          min_delta=0.01,\n",
    "                          patience=4,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True,\n",
    "                          mode='max')\n",
    "\n",
    "# Defining checkpoint callback\n",
    "filepath = './vgg_ft.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath,\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=0,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "\n",
    "# fine-tune the model\n",
    "model.fit(train_generator,\n",
    "          epochs=epochs,\n",
    "          validation_data=validation_generator,\n",
    "          callbacks=[earlystop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PbiGouJrpAE1",
    "outputId": "3af897c2-4d3e-46ae-9266-45b912fe4682"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oleg\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1877: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.481204</td>\n",
       "      <td>0.8125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy\n",
       "0  0.481204    0.8125"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([model.evaluate(test_generator)], columns=model.metrics_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can see that the usage of a pre-trained model is beneficial"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Additional_task.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
